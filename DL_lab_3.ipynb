{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rwh_EA3r3uXX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class DenseLayer:\n",
        "  def __init__(self,n_features,n_neurons):\n",
        "    self.weights = 0.01 * torch.rand(n_neurons,n_features)\n",
        "    self.bias = torch.zeros((1,n_neurons))\n",
        "  def forward(self, inputs):\n",
        "    self.output = torch.matmul(inputs,self.weights.T) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Input = torch.rand(32,5)\n",
        "\n",
        "print(\"Input:\",Input)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKjitX4a4wtX",
        "outputId": "c1799f75-df77-467c-a07e-10104d2c6b0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: tensor([[9.3456e-02, 6.1757e-01, 7.2394e-01, 1.5882e-01, 7.4983e-02],\n",
            "        [5.0465e-01, 5.1937e-02, 5.4164e-01, 1.4493e-01, 6.1753e-01],\n",
            "        [2.3083e-01, 4.1984e-02, 3.9531e-01, 2.2809e-01, 9.9939e-01],\n",
            "        [6.0106e-01, 9.6725e-01, 3.1692e-01, 2.4101e-01, 3.3429e-01],\n",
            "        [4.9620e-01, 5.5855e-01, 2.5494e-01, 4.9541e-01, 9.7531e-01],\n",
            "        [2.7876e-01, 3.5895e-01, 8.0839e-01, 7.5531e-01, 8.9459e-01],\n",
            "        [9.7368e-01, 2.2116e-01, 6.4266e-01, 7.8215e-01, 9.4693e-01],\n",
            "        [2.9324e-01, 7.9394e-01, 2.6614e-02, 8.8191e-01, 7.1745e-01],\n",
            "        [3.8250e-01, 9.8587e-01, 5.2745e-01, 8.0244e-01, 7.0649e-01],\n",
            "        [4.5128e-01, 9.5849e-01, 7.7867e-01, 4.0874e-01, 2.8190e-01],\n",
            "        [3.4790e-01, 4.7368e-01, 2.6599e-01, 8.7493e-01, 2.0592e-01],\n",
            "        [3.1629e-01, 8.2024e-01, 3.4666e-01, 5.2852e-01, 3.8047e-01],\n",
            "        [3.8161e-01, 5.4515e-01, 5.2624e-02, 8.1635e-01, 6.7609e-01],\n",
            "        [7.1167e-01, 3.9738e-01, 7.1999e-01, 4.0845e-01, 8.8500e-01],\n",
            "        [5.9680e-01, 8.5194e-01, 6.3786e-01, 6.1364e-02, 2.0681e-01],\n",
            "        [3.9931e-01, 7.1058e-01, 3.5996e-01, 6.1636e-01, 9.5164e-01],\n",
            "        [5.9854e-01, 6.4670e-01, 5.3234e-01, 7.9193e-01, 2.6144e-01],\n",
            "        [9.3546e-01, 1.5536e-02, 9.3039e-01, 2.8581e-01, 5.1520e-01],\n",
            "        [9.1121e-01, 3.9674e-02, 5.1358e-01, 3.7964e-01, 3.1959e-01],\n",
            "        [4.8691e-01, 2.0519e-02, 4.8165e-01, 7.8315e-01, 4.4148e-01],\n",
            "        [2.2123e-01, 3.7429e-01, 1.7155e-01, 8.4546e-02, 7.0215e-01],\n",
            "        [9.0431e-01, 9.1196e-01, 1.0999e-01, 2.3589e-02, 5.7667e-01],\n",
            "        [1.9785e-02, 8.9911e-01, 1.1557e-01, 6.3330e-02, 9.7809e-01],\n",
            "        [1.1031e-01, 6.0964e-01, 5.7117e-01, 1.7470e-01, 4.6487e-01],\n",
            "        [4.0123e-01, 5.5308e-02, 1.9344e-01, 2.0166e-01, 5.9686e-01],\n",
            "        [2.5723e-01, 9.2609e-01, 7.8772e-01, 5.7580e-01, 1.9393e-01],\n",
            "        [2.1420e-03, 8.2784e-01, 4.3392e-04, 2.0479e-01, 7.0770e-01],\n",
            "        [3.1542e-01, 3.9173e-01, 2.7826e-01, 9.4322e-01, 7.5140e-01],\n",
            "        [5.8509e-01, 6.1337e-01, 2.8717e-01, 2.9466e-01, 3.5836e-01],\n",
            "        [9.3453e-01, 7.3683e-01, 4.3411e-01, 5.3281e-01, 8.2313e-01],\n",
            "        [1.1239e-01, 4.8150e-01, 4.5105e-01, 4.3631e-01, 9.6886e-01],\n",
            "        [5.5692e-01, 9.9184e-01, 2.7057e-02, 9.0081e-01, 6.7297e-01]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FirstLayer = DenseLayer(5,16)\n",
        "\n",
        "FirstLayer.forward(Input)\n",
        "\n",
        "print(\"First Layer result:\",FirstLayer.output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7Sr2fxq50BQ",
        "outputId": "a2918573-c04b-4ed3-be3d-c841e2c15865"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Layer result: tensor([[0.0088, 0.0135, 0.0073, 0.0071, 0.0072, 0.0048, 0.0058, 0.0101, 0.0069,\n",
            "         0.0088, 0.0084, 0.0093, 0.0099, 0.0093, 0.0072, 0.0098],\n",
            "        [0.0133, 0.0143, 0.0064, 0.0102, 0.0073, 0.0089, 0.0078, 0.0140, 0.0067,\n",
            "         0.0077, 0.0102, 0.0100, 0.0083, 0.0146, 0.0075, 0.0103],\n",
            "        [0.0135, 0.0152, 0.0080, 0.0134, 0.0095, 0.0088, 0.0115, 0.0146, 0.0073,\n",
            "         0.0107, 0.0085, 0.0098, 0.0111, 0.0156, 0.0049, 0.0121],\n",
            "        [0.0133, 0.0191, 0.0133, 0.0141, 0.0136, 0.0114, 0.0097, 0.0177, 0.0099,\n",
            "         0.0142, 0.0095, 0.0113, 0.0147, 0.0143, 0.0077, 0.0132],\n",
            "        [0.0177, 0.0211, 0.0147, 0.0196, 0.0155, 0.0139, 0.0154, 0.0209, 0.0112,\n",
            "         0.0171, 0.0105, 0.0124, 0.0169, 0.0196, 0.0061, 0.0154],\n",
            "        [0.0208, 0.0229, 0.0142, 0.0197, 0.0140, 0.0132, 0.0166, 0.0211, 0.0127,\n",
            "         0.0175, 0.0146, 0.0151, 0.0174, 0.0223, 0.0089, 0.0164],\n",
            "        [0.0254, 0.0251, 0.0155, 0.0221, 0.0152, 0.0189, 0.0165, 0.0262, 0.0137,\n",
            "         0.0170, 0.0177, 0.0165, 0.0164, 0.0268, 0.0115, 0.0165],\n",
            "        [0.0163, 0.0193, 0.0174, 0.0209, 0.0165, 0.0136, 0.0166, 0.0191, 0.0119,\n",
            "         0.0195, 0.0084, 0.0102, 0.0183, 0.0172, 0.0034, 0.0133],\n",
            "        [0.0201, 0.0253, 0.0189, 0.0222, 0.0183, 0.0150, 0.0176, 0.0232, 0.0145,\n",
            "         0.0216, 0.0132, 0.0151, 0.0214, 0.0214, 0.0082, 0.0178],\n",
            "        [0.0162, 0.0221, 0.0140, 0.0150, 0.0137, 0.0114, 0.0112, 0.0191, 0.0118,\n",
            "         0.0157, 0.0131, 0.0143, 0.0165, 0.0170, 0.0104, 0.0153],\n",
            "        [0.0139, 0.0139, 0.0124, 0.0144, 0.0103, 0.0108, 0.0109, 0.0141, 0.0094,\n",
            "         0.0131, 0.0091, 0.0084, 0.0118, 0.0137, 0.0050, 0.0082],\n",
            "        [0.0135, 0.0178, 0.0136, 0.0151, 0.0131, 0.0105, 0.0116, 0.0162, 0.0102,\n",
            "         0.0152, 0.0091, 0.0105, 0.0151, 0.0142, 0.0060, 0.0123],\n",
            "        [0.0158, 0.0171, 0.0149, 0.0188, 0.0141, 0.0130, 0.0147, 0.0177, 0.0106,\n",
            "         0.0166, 0.0085, 0.0094, 0.0153, 0.0166, 0.0038, 0.0115],\n",
            "        [0.0209, 0.0237, 0.0133, 0.0183, 0.0140, 0.0148, 0.0140, 0.0229, 0.0119,\n",
            "         0.0153, 0.0152, 0.0156, 0.0158, 0.0227, 0.0108, 0.0168],\n",
            "        [0.0129, 0.0188, 0.0107, 0.0109, 0.0112, 0.0097, 0.0073, 0.0165, 0.0091,\n",
            "         0.0114, 0.0111, 0.0123, 0.0127, 0.0138, 0.0100, 0.0131],\n",
            "        [0.0188, 0.0231, 0.0165, 0.0211, 0.0169, 0.0142, 0.0169, 0.0219, 0.0126,\n",
            "         0.0193, 0.0114, 0.0136, 0.0192, 0.0206, 0.0066, 0.0168],\n",
            "        [0.0178, 0.0195, 0.0144, 0.0165, 0.0127, 0.0135, 0.0121, 0.0189, 0.0117,\n",
            "         0.0152, 0.0130, 0.0124, 0.0145, 0.0179, 0.0089, 0.0120],\n",
            "        [0.0197, 0.0195, 0.0081, 0.0124, 0.0084, 0.0132, 0.0085, 0.0195, 0.0094,\n",
            "         0.0087, 0.0166, 0.0145, 0.0093, 0.0206, 0.0131, 0.0126],\n",
            "        [0.0160, 0.0146, 0.0077, 0.0109, 0.0072, 0.0122, 0.0068, 0.0160, 0.0077,\n",
            "         0.0073, 0.0127, 0.0105, 0.0070, 0.0163, 0.0096, 0.0085],\n",
            "        [0.0164, 0.0143, 0.0097, 0.0139, 0.0083, 0.0115, 0.0107, 0.0151, 0.0088,\n",
            "         0.0107, 0.0116, 0.0098, 0.0096, 0.0166, 0.0069, 0.0086],\n",
            "        [0.0094, 0.0130, 0.0080, 0.0110, 0.0094, 0.0071, 0.0089, 0.0121, 0.0062,\n",
            "         0.0099, 0.0055, 0.0076, 0.0104, 0.0111, 0.0035, 0.0103],\n",
            "        [0.0142, 0.0201, 0.0133, 0.0151, 0.0148, 0.0135, 0.0098, 0.0200, 0.0096,\n",
            "         0.0138, 0.0094, 0.0115, 0.0145, 0.0158, 0.0079, 0.0142],\n",
            "        [0.0104, 0.0185, 0.0130, 0.0159, 0.0152, 0.0082, 0.0135, 0.0158, 0.0089,\n",
            "         0.0164, 0.0049, 0.0098, 0.0174, 0.0130, 0.0027, 0.0156],\n",
            "        [0.0108, 0.0160, 0.0093, 0.0109, 0.0100, 0.0067, 0.0091, 0.0130, 0.0080,\n",
            "         0.0116, 0.0083, 0.0102, 0.0125, 0.0121, 0.0062, 0.0122],\n",
            "        [0.0104, 0.0108, 0.0062, 0.0098, 0.0069, 0.0079, 0.0075, 0.0114, 0.0054,\n",
            "         0.0073, 0.0067, 0.0069, 0.0072, 0.0116, 0.0042, 0.0078],\n",
            "        [0.0153, 0.0206, 0.0140, 0.0146, 0.0129, 0.0102, 0.0115, 0.0172, 0.0117,\n",
            "         0.0159, 0.0123, 0.0133, 0.0163, 0.0158, 0.0092, 0.0140],\n",
            "        [0.0085, 0.0148, 0.0119, 0.0138, 0.0131, 0.0072, 0.0116, 0.0129, 0.0077,\n",
            "         0.0145, 0.0036, 0.0075, 0.0148, 0.0102, 0.0015, 0.0121],\n",
            "        [0.0180, 0.0184, 0.0149, 0.0197, 0.0138, 0.0134, 0.0160, 0.0186, 0.0114,\n",
            "         0.0171, 0.0106, 0.0109, 0.0158, 0.0188, 0.0049, 0.0124],\n",
            "        [0.0128, 0.0160, 0.0109, 0.0125, 0.0109, 0.0106, 0.0087, 0.0156, 0.0085,\n",
            "         0.0115, 0.0090, 0.0097, 0.0116, 0.0135, 0.0068, 0.0107],\n",
            "        [0.0220, 0.0256, 0.0171, 0.0215, 0.0175, 0.0178, 0.0155, 0.0257, 0.0135,\n",
            "         0.0185, 0.0149, 0.0158, 0.0185, 0.0236, 0.0104, 0.0175],\n",
            "        [0.0154, 0.0195, 0.0126, 0.0173, 0.0136, 0.0102, 0.0148, 0.0175, 0.0102,\n",
            "         0.0160, 0.0094, 0.0118, 0.0163, 0.0174, 0.0053, 0.0151],\n",
            "        [0.0186, 0.0223, 0.0197, 0.0229, 0.0187, 0.0163, 0.0173, 0.0225, 0.0135,\n",
            "         0.0213, 0.0103, 0.0120, 0.0201, 0.0195, 0.0053, 0.0149]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SecondLayer = DenseLayer(16,16)\n",
        "\n",
        "SecondLayer.forward(FirstLayer.output)\n",
        "\n",
        "\n",
        "print(\"Second Layer result:\",SecondLayer.output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkOhl3Uq6RRV",
        "outputId": "4de57ae8-46f2-4a24-a325-f73b4bd16bf2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second Layer result: tensor([[0.0006, 0.0007, 0.0005, 0.0007, 0.0007, 0.0006, 0.0005, 0.0009, 0.0007,\n",
            "         0.0006, 0.0008, 0.0006, 0.0007, 0.0007, 0.0006, 0.0005],\n",
            "        [0.0007, 0.0008, 0.0006, 0.0009, 0.0008, 0.0007, 0.0006, 0.0010, 0.0008,\n",
            "         0.0008, 0.0009, 0.0007, 0.0008, 0.0008, 0.0008, 0.0006],\n",
            "        [0.0008, 0.0009, 0.0006, 0.0009, 0.0008, 0.0007, 0.0006, 0.0011, 0.0008,\n",
            "         0.0008, 0.0011, 0.0008, 0.0009, 0.0009, 0.0008, 0.0006],\n",
            "        [0.0010, 0.0011, 0.0007, 0.0011, 0.0010, 0.0009, 0.0008, 0.0013, 0.0010,\n",
            "         0.0009, 0.0013, 0.0009, 0.0010, 0.0011, 0.0010, 0.0008],\n",
            "        [0.0012, 0.0013, 0.0009, 0.0013, 0.0012, 0.0010, 0.0009, 0.0015, 0.0012,\n",
            "         0.0011, 0.0015, 0.0011, 0.0012, 0.0013, 0.0012, 0.0009],\n",
            "        [0.0013, 0.0014, 0.0010, 0.0014, 0.0013, 0.0011, 0.0010, 0.0017, 0.0013,\n",
            "         0.0013, 0.0016, 0.0012, 0.0013, 0.0014, 0.0012, 0.0010],\n",
            "        [0.0014, 0.0016, 0.0011, 0.0016, 0.0014, 0.0012, 0.0011, 0.0019, 0.0015,\n",
            "         0.0014, 0.0018, 0.0014, 0.0016, 0.0015, 0.0014, 0.0011],\n",
            "        [0.0012, 0.0013, 0.0009, 0.0012, 0.0012, 0.0010, 0.0009, 0.0015, 0.0012,\n",
            "         0.0011, 0.0015, 0.0011, 0.0011, 0.0012, 0.0011, 0.0009],\n",
            "        [0.0014, 0.0015, 0.0010, 0.0015, 0.0014, 0.0012, 0.0011, 0.0018, 0.0014,\n",
            "         0.0013, 0.0018, 0.0013, 0.0014, 0.0015, 0.0014, 0.0011],\n",
            "        [0.0011, 0.0012, 0.0008, 0.0013, 0.0012, 0.0010, 0.0009, 0.0015, 0.0012,\n",
            "         0.0011, 0.0014, 0.0011, 0.0012, 0.0012, 0.0011, 0.0009],\n",
            "        [0.0009, 0.0009, 0.0006, 0.0009, 0.0009, 0.0007, 0.0007, 0.0011, 0.0009,\n",
            "         0.0008, 0.0011, 0.0008, 0.0009, 0.0009, 0.0008, 0.0007],\n",
            "        [0.0010, 0.0011, 0.0007, 0.0011, 0.0010, 0.0008, 0.0007, 0.0013, 0.0010,\n",
            "         0.0009, 0.0012, 0.0009, 0.0010, 0.0011, 0.0009, 0.0008],\n",
            "        [0.0011, 0.0011, 0.0008, 0.0011, 0.0010, 0.0009, 0.0008, 0.0013, 0.0011,\n",
            "         0.0010, 0.0013, 0.0010, 0.0011, 0.0011, 0.0010, 0.0008],\n",
            "        [0.0012, 0.0014, 0.0010, 0.0014, 0.0013, 0.0011, 0.0010, 0.0017, 0.0013,\n",
            "         0.0013, 0.0016, 0.0012, 0.0014, 0.0014, 0.0013, 0.0010],\n",
            "        [0.0009, 0.0010, 0.0007, 0.0011, 0.0009, 0.0008, 0.0007, 0.0012, 0.0010,\n",
            "         0.0009, 0.0011, 0.0009, 0.0010, 0.0010, 0.0009, 0.0007],\n",
            "        [0.0013, 0.0014, 0.0010, 0.0014, 0.0013, 0.0011, 0.0010, 0.0017, 0.0013,\n",
            "         0.0012, 0.0016, 0.0012, 0.0013, 0.0014, 0.0012, 0.0010],\n",
            "        [0.0011, 0.0012, 0.0008, 0.0012, 0.0011, 0.0009, 0.0008, 0.0014, 0.0011,\n",
            "         0.0011, 0.0014, 0.0011, 0.0012, 0.0012, 0.0011, 0.0008],\n",
            "        [0.0009, 0.0011, 0.0008, 0.0012, 0.0010, 0.0009, 0.0008, 0.0014, 0.0010,\n",
            "         0.0011, 0.0013, 0.0010, 0.0012, 0.0011, 0.0011, 0.0007],\n",
            "        [0.0007, 0.0009, 0.0007, 0.0010, 0.0008, 0.0007, 0.0006, 0.0011, 0.0008,\n",
            "         0.0008, 0.0010, 0.0008, 0.0009, 0.0009, 0.0009, 0.0006],\n",
            "        [0.0008, 0.0010, 0.0007, 0.0010, 0.0009, 0.0007, 0.0007, 0.0011, 0.0009,\n",
            "         0.0009, 0.0011, 0.0008, 0.0009, 0.0009, 0.0009, 0.0006],\n",
            "        [0.0007, 0.0007, 0.0005, 0.0007, 0.0007, 0.0006, 0.0005, 0.0009, 0.0007,\n",
            "         0.0007, 0.0009, 0.0007, 0.0007, 0.0007, 0.0007, 0.0005],\n",
            "        [0.0010, 0.0011, 0.0008, 0.0012, 0.0011, 0.0009, 0.0008, 0.0014, 0.0011,\n",
            "         0.0010, 0.0013, 0.0010, 0.0011, 0.0011, 0.0010, 0.0008],\n",
            "        [0.0010, 0.0010, 0.0007, 0.0010, 0.0010, 0.0008, 0.0007, 0.0012, 0.0010,\n",
            "         0.0009, 0.0012, 0.0009, 0.0009, 0.0010, 0.0009, 0.0008],\n",
            "        [0.0008, 0.0009, 0.0006, 0.0009, 0.0008, 0.0007, 0.0006, 0.0010, 0.0008,\n",
            "         0.0008, 0.0010, 0.0008, 0.0008, 0.0009, 0.0008, 0.0006],\n",
            "        [0.0006, 0.0007, 0.0005, 0.0007, 0.0006, 0.0005, 0.0005, 0.0008, 0.0006,\n",
            "         0.0006, 0.0008, 0.0006, 0.0007, 0.0007, 0.0006, 0.0005],\n",
            "        [0.0011, 0.0012, 0.0008, 0.0012, 0.0011, 0.0009, 0.0008, 0.0014, 0.0011,\n",
            "         0.0010, 0.0013, 0.0010, 0.0011, 0.0012, 0.0010, 0.0008],\n",
            "        [0.0009, 0.0009, 0.0006, 0.0008, 0.0008, 0.0007, 0.0006, 0.0010, 0.0008,\n",
            "         0.0007, 0.0010, 0.0008, 0.0008, 0.0009, 0.0007, 0.0007],\n",
            "        [0.0012, 0.0012, 0.0008, 0.0012, 0.0011, 0.0009, 0.0008, 0.0014, 0.0011,\n",
            "         0.0011, 0.0014, 0.0011, 0.0011, 0.0012, 0.0011, 0.0009],\n",
            "        [0.0009, 0.0009, 0.0006, 0.0010, 0.0009, 0.0007, 0.0007, 0.0011, 0.0009,\n",
            "         0.0008, 0.0011, 0.0008, 0.0009, 0.0009, 0.0009, 0.0007],\n",
            "        [0.0014, 0.0016, 0.0011, 0.0016, 0.0014, 0.0012, 0.0011, 0.0018, 0.0015,\n",
            "         0.0014, 0.0018, 0.0014, 0.0015, 0.0015, 0.0014, 0.0011],\n",
            "        [0.0011, 0.0012, 0.0008, 0.0012, 0.0011, 0.0009, 0.0008, 0.0014, 0.0011,\n",
            "         0.0010, 0.0014, 0.0010, 0.0011, 0.0011, 0.0010, 0.0008],\n",
            "        [0.0014, 0.0014, 0.0010, 0.0014, 0.0013, 0.0011, 0.0010, 0.0017, 0.0014,\n",
            "         0.0012, 0.0017, 0.0012, 0.0013, 0.0014, 0.0013, 0.0010]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ThridLayer = DenseLayer(16,16)\n",
        "\n",
        "ThridLayer.forward(SecondLayer.output)\n",
        "\n",
        "\n",
        "print(\"Thrid Layer result:\",ThridLayer.output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND4U1jkk6hH4",
        "outputId": "9f13a7ba-8da6-4e59-9fbc-c37029c7b9f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thrid Layer result: tensor([[4.0157e-05, 4.4021e-05, 5.9473e-05, 4.4114e-05, 4.5663e-05, 4.8851e-05,\n",
            "         5.1034e-05, 5.8962e-05, 4.9257e-05, 5.3377e-05, 4.0512e-05, 4.8900e-05,\n",
            "         5.4470e-05, 6.8516e-05, 5.1159e-05, 5.3917e-05],\n",
            "        [4.7529e-05, 5.2309e-05, 6.9917e-05, 5.2095e-05, 5.3673e-05, 5.7414e-05,\n",
            "         6.0269e-05, 6.9730e-05, 5.8095e-05, 6.3268e-05, 4.7901e-05, 5.7787e-05,\n",
            "         6.4415e-05, 8.0563e-05, 6.0609e-05, 6.3442e-05],\n",
            "        [5.1864e-05, 5.6719e-05, 7.5930e-05, 5.7501e-05, 5.8831e-05, 6.3030e-05,\n",
            "         6.5774e-05, 7.6067e-05, 6.4106e-05, 6.8818e-05, 5.2450e-05, 6.3038e-05,\n",
            "         7.0814e-05, 8.8127e-05, 6.5970e-05, 6.9087e-05],\n",
            "        [6.1775e-05, 6.7938e-05, 9.1189e-05, 6.8332e-05, 7.0481e-05, 7.5667e-05,\n",
            "         7.8392e-05, 9.0782e-05, 7.6366e-05, 8.1879e-05, 6.2462e-05, 7.5225e-05,\n",
            "         8.4272e-05, 1.0513e-04, 7.8544e-05, 8.2817e-05],\n",
            "        [7.3671e-05, 8.0802e-05, 1.0809e-04, 8.1905e-05, 8.3947e-05, 9.0116e-05,\n",
            "         9.3466e-05, 1.0819e-04, 9.1421e-05, 9.7589e-05, 7.4611e-05, 8.9627e-05,\n",
            "         1.0085e-04, 1.2526e-04, 9.3623e-05, 9.8380e-05],\n",
            "        [7.9410e-05, 8.7106e-05, 1.1658e-04, 8.7911e-05, 9.0219e-05, 9.6645e-05,\n",
            "         1.0084e-04, 1.1659e-04, 9.8041e-05, 1.0540e-04, 8.0339e-05, 9.6595e-05,\n",
            "         1.0838e-04, 1.3504e-04, 1.0108e-04, 1.0605e-04],\n",
            "        [9.0134e-05, 9.9405e-05, 1.3240e-04, 9.9312e-05, 1.0214e-04, 1.0944e-04,\n",
            "         1.1435e-04, 1.3238e-04, 1.1078e-04, 1.1978e-04, 9.1061e-05, 1.0963e-04,\n",
            "         1.2271e-04, 1.5277e-04, 1.1484e-04, 1.2034e-04],\n",
            "        [7.1166e-05, 7.8054e-05, 1.0431e-04, 7.9675e-05, 8.1499e-05, 8.7634e-05,\n",
            "         9.0372e-05, 1.0462e-04, 8.8965e-05, 9.4052e-05, 7.2288e-05, 8.6629e-05,\n",
            "         9.7951e-05, 1.2119e-04, 9.0318e-05, 9.5119e-05],\n",
            "        [8.6962e-05, 9.5406e-05, 1.2787e-04, 9.6670e-05, 9.9267e-05, 1.0654e-04,\n",
            "         1.1044e-04, 1.2778e-04, 1.0793e-04, 1.1519e-04, 8.8105e-05, 1.0586e-04,\n",
            "         1.1905e-04, 1.4809e-04, 1.1053e-04, 1.1636e-04],\n",
            "        [7.0668e-05, 7.7668e-05, 1.0438e-04, 7.7921e-05, 8.0451e-05, 8.6219e-05,\n",
            "         8.9747e-05, 1.0382e-04, 8.7022e-05, 9.3810e-05, 7.1395e-05, 8.6044e-05,\n",
            "         9.6175e-05, 1.2033e-04, 8.9958e-05, 9.4751e-05],\n",
            "        [5.3118e-05, 5.8591e-05, 7.8049e-05, 5.8997e-05, 6.0607e-05, 6.5061e-05,\n",
            "         6.7502e-05, 7.8130e-05, 6.5854e-05, 7.0396e-05, 5.3859e-05, 6.4675e-05,\n",
            "         7.2766e-05, 9.0272e-05, 6.7579e-05, 7.1078e-05],\n",
            "        [6.0460e-05, 6.6376e-05, 8.9014e-05, 6.7178e-05, 6.9068e-05, 7.4153e-05,\n",
            "         7.6783e-05, 8.8858e-05, 7.5031e-05, 8.0071e-05, 6.1247e-05, 7.3615e-05,\n",
            "         8.2747e-05, 1.0299e-04, 7.6839e-05, 8.0973e-05],\n",
            "        [6.4443e-05, 7.0803e-05, 9.4434e-05, 7.1960e-05, 7.3642e-05, 7.9144e-05,\n",
            "         8.1818e-05, 9.4731e-05, 8.0328e-05, 8.5253e-05, 6.5408e-05, 7.8430e-05,\n",
            "         8.8556e-05, 1.0959e-04, 8.1850e-05, 8.6082e-05],\n",
            "        [7.9712e-05, 8.7659e-05, 1.1725e-04, 8.7809e-05, 9.0374e-05, 9.6802e-05,\n",
            "         1.0113e-04, 1.1702e-04, 9.7971e-05, 1.0592e-04, 8.0486e-05, 9.6960e-05,\n",
            "         1.0842e-04, 1.3532e-04, 1.0153e-04, 1.0650e-04],\n",
            "        [5.7513e-05, 6.3301e-05, 8.5145e-05, 6.3113e-05, 6.5359e-05, 7.0024e-05,\n",
            "         7.2985e-05, 8.4472e-05, 7.0516e-05, 7.6425e-05, 5.7990e-05, 7.0028e-05,\n",
            "         7.7997e-05, 9.7840e-05, 7.3253e-05, 7.7169e-05],\n",
            "        [7.9848e-05, 8.7518e-05, 1.1720e-04, 8.8846e-05, 9.1072e-05, 9.7761e-05,\n",
            "         1.0135e-04, 1.1728e-04, 9.9172e-05, 1.0575e-04, 8.0898e-05, 9.7158e-05,\n",
            "         1.0937e-04, 1.3589e-04, 1.0146e-04, 1.0669e-04],\n",
            "        [6.8902e-05, 7.6018e-05, 1.0150e-04, 7.6082e-05, 7.8404e-05, 8.4075e-05,\n",
            "         8.7512e-05, 1.0129e-04, 8.4936e-05, 9.1457e-05, 6.9694e-05, 8.3884e-05,\n",
            "         9.3962e-05, 1.1705e-04, 8.7739e-05, 9.2258e-05],\n",
            "        [6.4974e-05, 7.1875e-05, 9.5843e-05, 7.0668e-05, 7.3138e-05, 7.8140e-05,\n",
            "         8.2407e-05, 9.5360e-05, 7.8806e-05, 8.6689e-05, 6.5350e-05, 7.9015e-05,\n",
            "         8.7642e-05, 1.0992e-04, 8.3018e-05, 8.6834e-05],\n",
            "        [5.1919e-05, 5.7623e-05, 7.6538e-05, 5.6576e-05, 5.8532e-05, 6.2620e-05,\n",
            "         6.5844e-05, 7.6263e-05, 6.3116e-05, 6.9214e-05, 5.2277e-05, 6.3156e-05,\n",
            "         7.0182e-05, 8.7721e-05, 6.6317e-05, 6.9396e-05],\n",
            "        [5.4532e-05, 6.0234e-05, 8.0034e-05, 6.0104e-05, 6.1781e-05, 6.6166e-05,\n",
            "         6.9247e-05, 8.0124e-05, 6.7012e-05, 7.2494e-05, 5.5137e-05, 6.6334e-05,\n",
            "         7.4296e-05, 9.2398e-05, 6.9524e-05, 7.2798e-05],\n",
            "        [4.2558e-05, 4.6504e-05, 6.2452e-05, 4.7355e-05, 4.8506e-05, 5.2066e-05,\n",
            "         5.3968e-05, 6.2456e-05, 5.2861e-05, 5.6353e-05, 4.3082e-05, 5.1765e-05,\n",
            "         5.8246e-05, 7.2457e-05, 5.4047e-05, 5.6819e-05],\n",
            "        [6.5111e-05, 7.1669e-05, 9.6036e-05, 7.1957e-05, 7.4191e-05, 7.9690e-05,\n",
            "         8.2531e-05, 9.5656e-05, 8.0430e-05, 8.6304e-05, 6.5788e-05, 7.9261e-05,\n",
            "         8.8765e-05, 1.1063e-04, 8.2773e-05, 8.7193e-05],\n",
            "        [5.8548e-05, 6.3650e-05, 8.5916e-05, 6.5672e-05, 6.7124e-05, 7.2165e-05,\n",
            "         7.4277e-05, 8.5936e-05, 7.3359e-05, 7.7298e-05, 5.9409e-05, 7.1246e-05,\n",
            "         8.0531e-05, 1.0005e-04, 7.4181e-05, 7.8256e-05],\n",
            "        [4.9616e-05, 5.4248e-05, 7.3123e-05, 5.4922e-05, 5.6517e-05, 6.0563e-05,\n",
            "         6.3000e-05, 7.2830e-05, 6.1312e-05, 6.5806e-05, 5.0161e-05, 6.0384e-05,\n",
            "         6.7652e-05, 8.4615e-05, 6.3097e-05, 6.6427e-05],\n",
            "        [3.8298e-05, 4.2116e-05, 5.6160e-05, 4.2306e-05, 4.3407e-05, 4.6529e-05,\n",
            "         4.8553e-05, 5.6213e-05, 4.7190e-05, 5.0853e-05, 3.8701e-05, 4.6564e-05,\n",
            "         5.2203e-05, 6.4927e-05, 4.8752e-05, 5.1063e-05],\n",
            "        [6.6834e-05, 7.3407e-05, 9.8662e-05, 7.3851e-05, 7.6178e-05, 8.1644e-05,\n",
            "         8.4931e-05, 9.8207e-05, 8.2463e-05, 8.8681e-05, 6.7589e-05, 8.1387e-05,\n",
            "         9.1101e-05, 1.1390e-04, 8.5061e-05, 8.9624e-05],\n",
            "        [4.8597e-05, 5.2884e-05, 7.1292e-05, 5.4666e-05, 5.5842e-05, 6.0090e-05,\n",
            "         6.1678e-05, 7.1376e-05, 6.1079e-05, 6.4095e-05, 4.9379e-05, 5.9156e-05,\n",
            "         6.7005e-05, 8.3071e-05, 6.1541e-05, 6.4990e-05],\n",
            "        [6.9327e-05, 7.6158e-05, 1.0156e-04, 7.7237e-05, 7.9054e-05, 8.4861e-05,\n",
            "         8.8042e-05, 1.0188e-04, 8.6170e-05, 9.1821e-05, 7.0319e-05, 8.4354e-05,\n",
            "         9.5110e-05, 1.1785e-04, 8.8129e-05, 9.2556e-05],\n",
            "        [5.3631e-05, 5.9077e-05, 7.9056e-05, 5.9223e-05, 6.1048e-05, 6.5504e-05,\n",
            "         6.8048e-05, 7.8805e-05, 6.6151e-05, 7.1149e-05, 5.4208e-05, 6.5286e-05,\n",
            "         7.3099e-05, 9.1124e-05, 6.8242e-05, 7.1810e-05],\n",
            "        [8.8203e-05, 9.7114e-05, 1.2978e-04, 9.7472e-05, 1.0030e-04, 1.0760e-04,\n",
            "         1.1189e-04, 1.2957e-04, 1.0883e-04, 1.1702e-04, 8.9169e-05, 1.0733e-04,\n",
            "         1.2028e-04, 1.4977e-04, 1.1224e-04, 1.1794e-04],\n",
            "        [6.5753e-05, 7.1838e-05, 9.6418e-05, 7.3220e-05, 7.4945e-05, 8.0392e-05,\n",
            "         8.3462e-05, 9.6514e-05, 8.1690e-05, 8.7090e-05, 6.6614e-05, 7.9980e-05,\n",
            "         9.0058e-05, 1.1201e-04, 8.3539e-05, 8.7782e-05],\n",
            "        [8.1283e-05, 8.9330e-05, 1.1934e-04, 9.0729e-05, 9.3011e-05, 1.0000e-04,\n",
            "         1.0320e-04, 1.1951e-04, 1.0134e-04, 1.0749e-04, 8.2483e-05, 9.8961e-05,\n",
            "         1.1166e-04, 1.3832e-04, 1.0321e-04, 1.0873e-04]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Output = DenseLayer(16,5)\n",
        "\n",
        "Output.forward(ThridLayer.output)\n",
        "\n",
        "print(\"Output of the 5 Class:\",Output.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh3NO7RR9y0x",
        "outputId": "71e03657-e65c-4897-bf3c-4e4d46eabdfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the 5 Class: tensor([[4.0819e-06, 3.7136e-06, 4.8023e-06, 3.4647e-06, 3.5458e-06],\n",
            "        [7.0062e-06, 6.3787e-06, 8.2420e-06, 5.9521e-06, 6.0838e-06],\n",
            "        [9.1739e-06, 8.3504e-06, 1.0795e-05, 7.7991e-06, 7.9618e-06],\n",
            "        [8.6149e-06, 7.8412e-06, 1.0136e-05, 7.3194e-06, 7.4796e-06],\n",
            "        [6.8116e-06, 6.2088e-06, 8.0138e-06, 5.7951e-06, 5.9185e-06],\n",
            "        [6.4982e-06, 5.9177e-06, 7.6447e-06, 5.5227e-06, 5.6453e-06],\n",
            "        [9.0739e-06, 8.2651e-06, 1.0676e-05, 7.7152e-06, 7.8806e-06],\n",
            "        [1.0014e-05, 9.1177e-06, 1.1783e-05, 8.5127e-06, 8.6941e-06],\n",
            "        [4.3023e-06, 3.9183e-06, 5.0612e-06, 3.6566e-06, 3.7352e-06],\n",
            "        [5.5739e-06, 5.0748e-06, 6.5599e-06, 4.7422e-06, 4.8379e-06],\n",
            "        [9.1979e-06, 8.3756e-06, 1.0824e-05, 7.8180e-06, 7.9885e-06],\n",
            "        [6.6226e-06, 6.0298e-06, 7.7927e-06, 5.6297e-06, 5.7501e-06],\n",
            "        [4.6036e-06, 4.1888e-06, 5.4161e-06, 3.9070e-06, 3.9997e-06],\n",
            "        [5.9091e-06, 5.3800e-06, 6.9514e-06, 5.0234e-06, 5.1297e-06],\n",
            "        [5.6386e-06, 5.1340e-06, 6.6343e-06, 4.7928e-06, 4.8954e-06],\n",
            "        [5.6430e-06, 5.1399e-06, 6.6402e-06, 4.8027e-06, 4.8976e-06],\n",
            "        [8.5788e-06, 7.8198e-06, 1.0095e-05, 7.3047e-06, 7.4477e-06],\n",
            "        [4.9079e-06, 4.4718e-06, 5.7770e-06, 4.1796e-06, 4.2591e-06],\n",
            "        [5.8281e-06, 5.3061e-06, 6.8543e-06, 4.9482e-06, 5.0644e-06],\n",
            "        [6.0119e-06, 5.4782e-06, 7.0745e-06, 5.1150e-06, 5.2222e-06],\n",
            "        [4.4593e-06, 4.0555e-06, 5.2446e-06, 3.7817e-06, 3.8747e-06],\n",
            "        [7.6321e-06, 6.9524e-06, 8.9816e-06, 6.4936e-06, 6.6257e-06],\n",
            "        [4.2931e-06, 3.9079e-06, 5.0512e-06, 3.6499e-06, 3.7265e-06],\n",
            "        [4.9112e-06, 4.4730e-06, 5.7787e-06, 4.1752e-06, 4.2650e-06],\n",
            "        [5.5358e-06, 5.0370e-06, 6.5133e-06, 4.7010e-06, 4.8068e-06],\n",
            "        [5.8586e-06, 5.3333e-06, 6.8939e-06, 4.9815e-06, 5.0852e-06],\n",
            "        [5.5328e-06, 5.0395e-06, 6.5116e-06, 4.7105e-06, 4.7997e-06],\n",
            "        [7.5309e-06, 6.8593e-06, 8.8623e-06, 6.4087e-06, 6.5359e-06],\n",
            "        [4.4690e-06, 4.0749e-06, 5.2570e-06, 3.8010e-06, 3.8849e-06],\n",
            "        [7.7278e-06, 7.0407e-06, 9.0922e-06, 6.5749e-06, 6.7099e-06],\n",
            "        [6.5310e-06, 5.9476e-06, 7.6842e-06, 5.5513e-06, 5.6741e-06],\n",
            "        [6.7439e-06, 6.1410e-06, 7.9350e-06, 5.7291e-06, 5.8587e-06]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activiation Functions Implementation\n"
      ],
      "metadata": {
        "id": "U8j4TsuqXjap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_Sigmoid:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        self.output = 1 / (1 + torch.exp(-inputs))\n",
        "        return self.output\n",
        "# ReLU activation\n",
        "class Activation_ReLU:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        self.output = torch.max(torch.zeros_like(inputs), inputs)\n",
        "        return self.output\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        exp_values = torch.exp(inputs - torch.max(inputs, dim=1, keepdim=True).values)\n",
        "        probabilities = exp_values / torch.sum(exp_values, dim=1, keepdim=True)\n",
        "        self.output = probabilities\n",
        "        return self.output\n"
      ],
      "metadata": {
        "id": "WnFxGueBXmgm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical Cross-Entropy Loss Implmentation"
      ],
      "metadata": {
        "id": "CQosQVFBX8FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_CategoricalCrossentropy:\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        epsilon = 1e-15\n",
        "        y_pred_clipped = torch.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "\n",
        "        log_likelihoods = -torch.log(y_pred_clipped[range(len(y_true)), y_true])\n",
        "        return torch.mean(log_likelihoods)"
      ],
      "metadata": {
        "id": "OPjh_KjTYBYz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NeuralNetwork with three layers and use RELU, SIGMOID , SOFTMAX activation on the layers**"
      ],
      "metadata": {
        "id": "nBBCAVIJazbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_features, hidden_neurons, output_classes):\n",
        "        self.layer1 = DenseLayer(input_features, hidden_neurons)\n",
        "        self.activation1 = Activation_ReLU()\n",
        "\n",
        "        self.layer2 = DenseLayer(hidden_neurons, hidden_neurons)\n",
        "        self.activation2 = Activation_Sigmoid()\n",
        "\n",
        "        self.layer3 = DenseLayer(hidden_neurons, hidden_neurons)\n",
        "        self.activation3 = Activation_Softmax()\n",
        "\n",
        "        self.output_layer = DenseLayer(hidden_neurons, output_classes)\n",
        "\n",
        "    def forward_pass(self, inputs):\n",
        "        self.layer1.forward(inputs)\n",
        "        self.activation1.forward(self.layer1.output)\n",
        "\n",
        "        self.layer2.forward(self.activation1.output)\n",
        "        self.activation2.forward(self.layer2.output)\n",
        "\n",
        "        self.layer3.forward(self.activation2.output)\n",
        "        self.activation3.forward(self.layer3.output)\n",
        "\n",
        "        self.output_layer.forward(self.activation3.output)\n",
        "\n",
        "    def calculate_loss(self, true_labels):\n",
        "        categorical_loss = Loss_CategoricalCrossentropy()\n",
        "        loss = categorical_loss.forward(self.output_layer.output, true_labels)\n",
        "        return loss.item()\n",
        "\n",
        "    def calculate_accuracy(self, true_labels):\n",
        "        predictions = torch.argmax(self.output_layer.output, axis=1)\n",
        "        accuracy = torch.sum(predictions == true_labels).item() / len(true_labels)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "ppyDhcRJaLqt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define architecture\n",
        "input_features = 4\n",
        "hidden_neurons = 18\n",
        "output_classes = 3\n",
        "\n",
        "# Create the neural network\n",
        "model = NeuralNetwork(input_features, hidden_neurons, output_classes)\n",
        "\n",
        "# Generate random input\n",
        "inputs = torch.rand(32, input_features)\n",
        "true_labels = torch.randint(0, output_classes, (32,))\n",
        "\n",
        "# Forward pass\n",
        "model.forward_pass(inputs)\n",
        "\n",
        "# Calculate loss\n",
        "loss = model.calculate_loss(true_labels)\n",
        "print(\"Categorical Cross-Entropy Loss:\", loss)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = model.calculate_accuracy(true_labels)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp2hFXjqahOz",
        "outputId": "758aa0a0-529b-4dd8-c94a-f3a7f605a439"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical Cross-Entropy Loss: 5.253213405609131\n",
            "Accuracy: 0.28125\n"
          ]
        }
      ]
    }
  ]
}